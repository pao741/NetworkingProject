{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "toxic-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display \n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import requests\n",
    "import datetime\n",
    "import traceback\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "from datetime import date, timedelta\n",
    "# from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "alike-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset():\n",
    "    cwd = \"/home/pao741/Documents/ICCS/Networking/chain_networks\"\n",
    "    saving_dir = \"/home/pao741/Documents/ICCS/Networking/formated_dataset\"\n",
    "    for filename in os.listdir(cwd):\n",
    "        with open(\"/home/pao741/Documents/ICCS/Networking/chain_networks/\"+filename, \"r\") as handle:\n",
    "            parsed = json.load(handle)\n",
    "        with open('/home/pao741/Documents/ICCS/Networking/formated_dataset/'+filename, 'w') as outfile:\n",
    "            json.dump(parsed, outfile, indent=4, sort_keys=True)\n",
    "        handle.close()\n",
    "        outfile.close()\n",
    "\n",
    "# format_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-merit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n",
      "funny\n",
      "AskReddit\n",
      "gaming\n",
      "aww\n",
      "Music\n",
      "pics\n",
      "science\n",
      "worldnews\n",
      "videos\n",
      "todayilearned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pao741/.local/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/pao741/.local/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies\n",
      "news\n",
      "Showerthoughts\n",
      "EarthPorn\n",
      "gifs\n",
      "IAmA\n",
      "dankmemes\n",
      "ComedyCemetery\n",
      "Jokes\n",
      "LifeProTips\n",
      "PrequelMemes\n",
      "terriblefacebookmemes\n",
      "books\n",
      "mildlyinteresting\n",
      "nottheonion\n",
      "DIY\n",
      "sports\n",
      "blogs\n",
      "space\n",
      "gadgets\n",
      "anime\n",
      "memes\n"
     ]
    }
   ],
   "source": [
    "# date stuff \n",
    "\n",
    "# date(year, month, date)\n",
    "api = PushshiftAPI()\n",
    "sdate = date(2020, 8, 28)   # start date\n",
    "edate = date(2021, 2, 28)   # end date\n",
    "subredditNames = [\"funny\",'AskReddit','gaming','aww','Music','pics','science','worldnews','videos','todayilearned',\n",
    "                 'movies','news','Showerthoughts','EarthPorn','gifs','IAmA','dankmemes','ComedyCemetery','Jokes',\n",
    "                  'LifeProTips','PrequelMemes','terriblefacebookmemes','books','mildlyinteresting','nottheonion','DIY',\n",
    "                 'sports','blogs','space','gadgets','anime','memes','Unexpected','YouShouldKnow','programming',\n",
    "                 'bestof','fatpeoplestories','pettyrevenge','WTF','cringepics','JusticePorn','cringe'\n",
    "                 , 'mildlyinfuriating','rage','leagueoflegends','pokemon','Minecraft','Games'\n",
    "                 ,'DotA2','starcraft','skyrim','mindcrack','arresteddevelopment','gameofthrones','doctorwho'\n",
    "                 ,'harrypotter','StarWars','DaftPunk','hiphopheads','geek','AdviceAnimals'\n",
    "                  ,'ImGoingToHellForThis','circlejerk','facepalm'\n",
    "                  ,'TrollXChromosomes','AnimalsBeingJerks','youtubehaiku','woahdude','awwnime'\n",
    "                 ,'PerfectTiming','itookapicture','AbandonedPorn','techsupportgore','askscience'\n",
    "                 ,'explainlikeimfive','cats','corgi','food','teenagers','lifehacks','Frugal','Fitness','Art'\n",
    "                 ,'loseit','RedditLaqueristas','Random_Acts_Of_Amazon','politics','TrueReddit','atheism','lgbt'\n",
    "                 ,'TwoXChromosomes','MensRights','nba','soccer','hockey','nfl','Android'\n",
    "                 ,'technology','PewdiepieSubmissions','apple']\n",
    "\n",
    "def allDateBetween(sdate, edate):\n",
    "    dates = []\n",
    "    delta = edate - sdate       # as timedelta\n",
    "    for i in range(delta.days + 1):\n",
    "        day = sdate + timedelta(days=i)\n",
    "        dates.append(day)\n",
    "    return dates\n",
    "\n",
    "def createFolder():\n",
    "    Path(\"scraped_data/submissions\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"scraped_data/comments\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "\n",
    "def scrapRedditSubmission(sdate,edate,subreddits):\n",
    "    createFolder()\n",
    "    dates = allDateBetween(sdate,edate)\n",
    "    for subreddit in subreddits:\n",
    "        print(subreddit)\n",
    "        result = pd.DataFrame()\n",
    "        for i in range(len(dates)-1):\n",
    "            gen = api.search_submissions(subreddit=subreddit,after=str(dates[i]),before=str(dates[i+1]),\n",
    "                                     sort=\"desc\",sort_type=\"score\",limit=6)\n",
    "            listed_gen = list(gen)\n",
    "            df = pd.DataFrame([thing.d_ for thing in listed_gen])\n",
    "            result = result.append(df)\n",
    "            \n",
    "        pickle.dump(result, open(\"scraped_data/submissions/\" + subreddit +  \".p\", \"wb\"))\n",
    "        \n",
    "    \n",
    "# dates = allDateBetween(sdate,edate)\n",
    "print(len(dates))\n",
    "# createFolder()\n",
    "\n",
    "scrapRedditSubmission(sdate,edate,subredditNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suburban-classification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain_networks\tProcess\\ dataset.ipynb\tReddit\\ scrapper.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "# def scrapeRedditCommentFromSubmission(ids):\n",
    "#     for cid in ids:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit847514be24ec4d948819d5806e0dbd89"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
